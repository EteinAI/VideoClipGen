# Path: vcg/speechsynthesis/tokenizer.py

import logging

from dataclasses import dataclass
from difflib import SequenceMatcher
from itertools import groupby

from pysubs2 import SSAEvent, SSAFile, SSAStyle, Alignment

from .utils import (
  split_sent_sep,
  has_left_punct,
  has_sent_sep,
  replace_sent_sep,
)


# HACK! hardcoded user preferences
# break into separate ssa event if the break is longer than this
ssa_break_ms = 200
# max number of characters per line
ssa_line_width = 12


# Reference
# https://github.com/tkarabela/pysubs2/blob/master/pysubs2/ssaevent.py
@dataclass
class Token:
  text: str = ''
  begin: int = -1
  end: int = -1
  omitted: bool = False
  tts_sent_break: bool = False
  # word token id, generated by TTS engine
  tts_tok_id: int = -1
  nlp_sent: bool = False
  nlp_tok_idx: int = -1


def tokenize(ts, sentence):
  '''
  Tokenize the sentence and attach timestamp info generated by TTS engine
  1. Use difflib.SequenceMatcher to match the sentence with the TTS result
  2. Create tokens while processing the match results
  3. Take tokenization result from TTS engine as strong references, since they
     are timestamped

  Args:
    ts: list of timestamped tokens generated by TTS engine
    sentence: the original sentence to be tokenized

  Returns:
    A list of Token objects representing the tokens in the input sentence,
    with timestamp information attached.
  '''

  # Concatenate the text of all timestamped tokens to create the full TTS result
  tts_text = ''.join([t['text'] for t in ts])
  sent_text = replace_sent_sep(sentence)
  logging.info(tts_text)
  logging.info(sent_text)

  # Use SequenceMatcher to match the TTS result with the original sentence
  s = SequenceMatcher(None, tts_text, sent_text)

  # Initialize indices and empty list of tokens
  ts_idx = 0  # index of current timestamp data in `ts`
  char_idx = 0  # index of current character in `sentence`
  tts_tok_id = 0  # id of current token generated by TTS engine
  tokens = []  # list of Token objects

  # Iterate through match results, creating tokens as needed
  for tag, i1, i2, j1, j2 in s.get_opcodes():
    logging.info('{:7}   a[{}:{}] --> b[{}:{}] {!r:>8} --> {!r}'.format(
      tag, i1, i2, j1, j2, tts_text[i1:i2], sent_text[j1:j2]))

    # tag == 'delete' means TTS engine introduces new token that not belong to
    # the origin sentence, which should be preserved as-is
    if tag == 'equal' or tag == 'delete':
      # TODO
      # use NLP to split the long sentence into tokens
      while char_idx < i2:
        t = ts[ts_idx]
        token = tokens.append(Token(
          text=t['text'],
          begin=t['begin_time'],
          end=t['end_time'],
          tts_tok_id=tts_tok_id,
        ))
        # advance all indices
        char_idx += len(t['text'])
        ts_idx += 1
        if ts_idx < len(ts) and t['end_index'] != ts[ts_idx]['begin_index']:
          tts_tok_id += 1
    elif tag == 'replace':
      # merge as one token
      # if the last tag was `equal`, there might be an overflow since t['text']
      # might be word. e.g. if the tts gives `top十佳` and the origin sentence
      # is `toop10佳`, the match result will be:
      # `{ equal: 'to', replace: ['p十','op10'], equal: '佳' }`
      # in this case, tss treat `top` as a word, and we should honor that
      # there for:
      # 1. when dealing with the first `equal` part, `top` will be taken as a
      #    token, though 'p' is not part of the equal tag, only 'to' is;
      # 2. when dealing with `replace`, the `p` of 'p十' will be skip since it
      #    has been taken by token `top` in the first step, `op10`(from the
      #    origin sentence) will be created as a token, which will take the
      #    `begin_time` and `end_time` of `十` (`p十` without `p`)

      # HACK!
      # split the sentence break into separate tokens if sentence[j1:j2] starts
      # or ends with sentence break
      texts = split_sent_sep(sent_text[j1:j2])

      for text in texts:
        token = Token(text)
        if has_sent_sep(text):
          # if a sentence break was found, append a new token to the list
          # no need to set `tts_sent_break` since all punctuations will be
          # dealt with later
          tokens.append(token)
        else:
          # treat the diff part as a single token, iterate through
          # ts[char_idx:i2] to update the begin and end time of this token
          while char_idx < i2:
            t = ts[ts_idx]
            if token.begin == -1:
              token.begin = t['begin_time']
            token.end = t['end_time']
            char_idx += len(t['text'])
            ts_idx += 1
          # append the token to the list only if the token is not empty
          if token.text != '':
            tokens.append(token)
    elif tag == 'insert':
      # append what was ignored by TTS engine as a new token with `omitted` set
      # to True
      tokens.append(Token(
        text=sent_text[j1:j2],
        omitted=True,
      ))
    else:
      raise RuntimeError(f'Unknown tag: {tag}')

  return tokens


# TODO utilize NLP methods to get better results
# e.g. https://hanlp.hankcs.com/demos/tok.html
def wording(tokens: list[Token]) -> list[list[Token]]:
  '''
  Assemble the tokens into words
  '''

  tg = [list(g) for _, g in groupby(tokens, lambda x: x.tts_tok_id)]
  print([''.join([x.text for x in g]) for g in tg])

  # HACK! TODO extract all `sentence breaks` into separate tokens

  # HACK! merge tokens without timestamp info with the timestamped ones
  for item, idx in [(t, i) for i, t in enumerate(tokens) if t.begin == -1]:
    if has_left_punct(item.text) and idx < len(tokens) - 1:
      # HACK! merge into the `right` token if left punctuation is found
      tokens[idx + 1].text = item.text + tokens[idx + 1].text
    elif has_sent_sep(item.text):
      # HACK! sentence breaks should be treated separately, not here
      # HACK! this should be done in `tokenize`, probably
      # TODO! '\n' should be treated differently than '\N'
      item.tts_sent_break = True
    elif idx > 0:
      # HACK! otherwise merge into the `left` token
      tokens[idx - 1].text = tokens[idx - 1].text + item.text

  # remove tokens without timestamp info since they all have been merged
  tokens = [t for t in tokens if t.begin != -1 or t.tts_sent_break]
  logging.info([t.text for t in tokens])

  # list of words, each item is a list of tokens
  lines: list[list[Token]] = []

  # HACK! group tokens by sentence breaks while dropping lines that contains a
  #       single line break only
  # TODO timestamp info should be taken into consider in the future
  sentences = [s for s in [list(g) for _, g in groupby(
      tokens, lambda x: x.tts_sent_break)] if not s[0].tts_sent_break]
  for s in sentences:
    # HACK! assemble words from tokens if they share the same tts_tok_id
    # TODO consider use NLP in the future
    lines.append([Token(
      text=''.join([i.text for i in w]),
      begin=w[0].begin,
      end=w[-1].end,
    ) for w in [list(g) for _, g in groupby(s, lambda x: x.tts_tok_id)]])

  return lines


def subtitle(lines: list[list[Token]], video_size, ):
  '''
  Generate subtitle from lines(list of words)
  '''

  def wrap(texts):
    total_len = sum([len(t) for t in texts])
    n = round(total_len / ssa_line_width)
    if n < 2:
      text = ''.join(texts)
    else:
      avg_line_width = round(total_len / n)
      length = 0
      text = ''
      for t in texts:
        if length > avg_line_width:
          text += '\\N'
          length = 0
        text += t
        length += len(t)

    logging.info(f'Lines: {text}')

    return text

  # video dimension
  width, height = video_size

  # TODO consider ASS in the future
  subtitle = SSAFile()
  subtitle.info.update({
    'PlayResX': width,
    'PlayResY': height,
  })
  subtitle.styles = {
    'top': SSAStyle(
      # HACK! alignment/fontname should
      alignment=Alignment.TOP_CENTER,
      fontname='PingFang SC',
      fontsize=round(width / ssa_line_width / 1.2),
      marginv=round(height / 8),
      # 1/10 of font size
      outline=width / ssa_line_width * 0.05,
      shadow=0.0,
    )
  }
  subtitle.events = [SSAEvent(
    start=words[0].begin,
    end=words[-1].end,
    text=wrap([w.text for w in words]),
    style='top',
  ) for words in lines]

  return subtitle
