{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de58fcd0-3379-4dd5-9610-1ae655c65bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint, pformat\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Get my_package directory path from Notebook\n",
    "parent_dir = str(Path().resolve().parents[1])\n",
    "\n",
    "# Add to sys.path\n",
    "if parent_dir not in sys.path:\n",
    "  sys.path.insert(0, parent_dir)\n",
    "\n",
    "pprint(sys.path)\n",
    "\n",
    "testdata_path = os.path.abspath(os.path.join(\n",
    "  str(sys.path[0]), '..', 'tests', 'data'))\n",
    "print(\n",
    "  f'Test data {\"exists\" if os.path.exists(testdata_path) else \"dosent exists!\"}: {testdata_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0789bd-6014-49ec-9a7e-851067c51dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(os.path.join(testdata_path, 'ssa'))\n",
    "ts_files = sorted([str(p) for p in path.glob('**/*.json')])\n",
    "summaries = [\n",
    "  '这145平米的样板间采用10多种色彩, 创造了和谐、高级的空间！',\n",
    "  '客餐厅采用无吊顶设计，左右清晰分区',\n",
    "  '墙面、顶面和门框采用黑色金属线条，营造简约现代感。',\n",
    "  '餐椅和抱枕的脏橘\"色贯\"穿客餐厅，避免了视觉上的割裂感:',\n",
    "  '选择雅琪诺悦动风华系列的“窗帘”，营造出<温馨>的氛围?',\n",
    "  '主卧和次卧都采用了不同的装饰《元素》, 呈现出不同的风格, 相应地丰富了整个空间',\n",
    "  \"恒洁通过“恒洁选品SHOW”和“设计圈”等工具，与装企合作，提升效率，享受合作共赢的喜悦。让你更轻松、更高效地完成家装。\",\n",
    "  \"恒洁T9PRO花洒荣膺“红鼎技术TOP10”大奖，恒洁“技术包”让你的卫浴更加智能、舒适。精确控温技术、长效阻垢技术、水净技术……细节成就卓越。\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dd36e0-d6a6-404b-8f67-5d1edd14d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def remove_punct(text):\n",
    "  return re.sub(r'[^\\w]', '', text)\n",
    "  # Define regex patterns for Chinese and English punctuation\n",
    "  symbol_pattern = '\\\\s+'\n",
    "  chinese_punct_pattern = '[\\u3000-\\u303f\\uFF00-\\uFFEF]'\n",
    "  english_punct_pattern = '[\\u0021-\\u002f\\u003a-\\u0040\\u005b-\\u0060\\u007b-\\u007e]'\n",
    "  cond = '|'.join([\n",
    "    symbol_pattern,\n",
    "    chinese_punct_pattern,\n",
    "    english_punct_pattern,\n",
    "  ])\n",
    "\n",
    "  # Remove Chinese and English punctuation using regex\n",
    "  return re.sub(cond, '', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3ea4b7e-203a-41ea-8c6a-e60fe8274194",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summaries' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m summaries:\n\u001b[1;32m      2\u001b[0m   \u001b[39m# print(remove_punct(s))\u001b[39;00m\n\u001b[1;32m      3\u001b[0m   \u001b[39m# print(s)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m   \u001b[39mprint\u001b[39m(re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[^\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw]\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, s))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'summaries' is not defined"
     ]
    }
   ],
   "source": [
    "for s in summaries:\n",
    "  # print(remove_punct(s))\n",
    "  # print(s)\n",
    "  print(re.sub(r'[^\\w]', '', s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79807e9a-275e-4c8d-b3bc-248f19e70e1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_ts(ts_files, summaries):\n",
    "  for f, s in zip(ts_files, summaries):\n",
    "    ts = json.load(open(f, 'r', encoding='utf-8'))['payload']['subtitles']\n",
    "    words = ['']\n",
    "    index = 0\n",
    "    for t in ts:\n",
    "      if t['begin_index'] == index:\n",
    "        words[-1] += t['text']\n",
    "      else:\n",
    "        words.append(t['text'])\n",
    "      index = t['end_index']\n",
    "\n",
    "    summary = remove_punct(s)\n",
    "    pointer = 0\n",
    "    ranges = [(0, 0)]\n",
    "    for w in words:\n",
    "      print(f'Working on {w}')\n",
    "      l = len(w)\n",
    "      if ranges[-1] is not None:\n",
    "        if w == summary[pointer:pointer + l]:\n",
    "          ranges.append((pointer, pointer + l))\n",
    "          pointer += l\n",
    "        else:\n",
    "          print(f'miss: {summary[pointer:]}')\n",
    "          ranges.append(None)\n",
    "      else:\n",
    "        if p := summary[pointer:].find(w):\n",
    "          print(f'found: {w} at {p+pointer}')\n",
    "          print(f'from {summary[pointer:]}')\n",
    "          pointer += p\n",
    "          ranges.append((pointer, pointer + l))\n",
    "          pointer += l\n",
    "\n",
    "    for i, r in enumerate(ranges):\n",
    "      if r is None:\n",
    "        start = ranges[i - 1][1] if i > 0 else 0\n",
    "        end = ranges[i + 1][0] if i < len(ranges) - 1 else len(summary)\n",
    "        ranges[i] = (start, end)\n",
    "\n",
    "    print(words)\n",
    "    print([summary[r[0]:r[1]] for r in ranges[1:] if r is not None])\n",
    "    print(f'summary: {s}')\n",
    "\n",
    "\n",
    "test_ts(ts_files, summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cdc299-7eb0-4c8b-bab1-c1e00eadc4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssa_break_ms = 200\n",
    "ssa_line_width = 12\n",
    "\n",
    "\n",
    "def tok(timestamps):\n",
    "  '''\n",
    "  Tokenize the timestamps into words\n",
    "  1. combine consecutive indexed characters into words\n",
    "  2. unless the timestamp of two characters are not consecutive\n",
    "  '''\n",
    "\n",
    "  # assemble words based on `begin_index` and `end_index`\n",
    "  words = [{'text': '', 'begin': 0, 'end': 0}]\n",
    "  index = 0\n",
    "  for t in timestamps:\n",
    "    # assuming the `begin_time`` of the first item is 0\n",
    "    if t['begin_index'] == index and t['begin_time'] - words[-1]['end'] < 50:\n",
    "      words[-1]['text'] += t['text']\n",
    "      words[-1]['end'] = t['end_time']\n",
    "    else:\n",
    "      words.append({\n",
    "        'text': t['text'],\n",
    "        'begin': t['begin_time'],\n",
    "        'end': t['end_time']\n",
    "      })\n",
    "    index = t['end_index']\n",
    "\n",
    "  return words\n",
    "\n",
    "\n",
    "def gen_subtitle(words):\n",
    "  lines = []\n",
    "\n",
    "  def new_line(begin):\n",
    "    lines.append({\n",
    "      'texts': [],\n",
    "      'start': begin,\n",
    "      'end': 0,\n",
    "    })\n",
    "    return lines[-1]\n",
    "\n",
    "  def wrap(texts):\n",
    "    total_len = sum([len(t) for t in texts])\n",
    "    n = round(total_len / ssa_line_width)\n",
    "    if n < 2:\n",
    "      return ''.join(texts)\n",
    "\n",
    "    avg_line_width = round(total_len / n)\n",
    "    length = 0\n",
    "    text = ''\n",
    "    for t in texts:\n",
    "      if length > avg_line_width:\n",
    "        text += '\\\\N'\n",
    "        length = 0\n",
    "      text += t\n",
    "      length += len(t)\n",
    "    return text\n",
    "\n",
    "  new_line(0)\n",
    "  for word in words:\n",
    "    if word['begin'] - lines[-1]['end'] > ssa_break_ms:\n",
    "      line = new_line(word['begin'])\n",
    "    elif word['begin'] - lines[-1]['end'] > 100 and sum([len(t) for t in line['texts']]) > ssa_line_width:\n",
    "      line = new_line(word['begin'])\n",
    "    else:\n",
    "      line = lines[-1]\n",
    "    line['end'] = word['end']\n",
    "    line['texts'].append(word['text'])\n",
    "\n",
    "  for line in lines:\n",
    "    print(wrap(line['texts']))\n",
    "\n",
    "\n",
    "def mapping(words, sentence):\n",
    "  '''\n",
    "  tokenize the sentence by aligning the summary sentence with given words\n",
    "  '''\n",
    "\n",
    "  print(f'sentence: {sentence}')\n",
    "  print(f'words: {pformat(words)}')\n",
    "\n",
    "  # matching all words from the beginning of the sentence\n",
    "  # skip the words that cannot be matched, fill the gap later\n",
    "  pointer = 0\n",
    "  ranges = [{'range': (0, 0), 'begin': 0, 'end': 0}]\n",
    "  for w in words:\n",
    "    l = len(w['text'])\n",
    "    if ranges[-1]['range'] is not None:\n",
    "      # sentence and words are currently matched before dealing with word w\n",
    "      if w['text'] == sentence[pointer:pointer + l]:\n",
    "        # w and sentence are matched at (pointer, pointer + l) of sentence\n",
    "        ranges.append({\n",
    "          'range': (pointer, pointer + l),\n",
    "          'begin': w['begin'],\n",
    "          'end': w['end'],\n",
    "        })\n",
    "        pointer += l\n",
    "      else:\n",
    "        # w and sentence are not matched, record the timestamp\n",
    "        ranges.append({\n",
    "          'range': None,\n",
    "          'begin': w['begin'],\n",
    "          'end': w['end'],\n",
    "        })\n",
    "    else:\n",
    "      # sentence and words are NOT matched before dealing with word w\n",
    "      if (p := sentence[pointer:].find(w['text'])) > -1:\n",
    "        # w and sentence are matched, at position `p` from `pointer`\n",
    "        print(w['text'])\n",
    "        print(p)\n",
    "        pointer += p\n",
    "        ranges.append({\n",
    "          'range': (pointer, pointer + l),\n",
    "          'begin': w['begin'],\n",
    "          'end': w['end'],\n",
    "        })\n",
    "        pointer += l\n",
    "      else:\n",
    "        # still no match, extend the timestamp\n",
    "        ranges[-1]['end'] = w['end']\n",
    "\n",
    "  # drop the placeholder\n",
    "  ranges = ranges[1:]\n",
    "  print(f'ranges: {pformat(ranges)}')\n",
    "\n",
    "  for i, r in enumerate(ranges):\n",
    "    if r['range'] is None:\n",
    "      start = ranges[i - 1]['range'][1] if i > 0 else 0\n",
    "      end = ranges[i + 1]['range'][0] if i < len(ranges) - 1 else len(sentence)\n",
    "      ranges[i]['range'] = (start, end)\n",
    "\n",
    "  tokens = [{\n",
    "    'text': sentence[r['range'][0]:r['range'][1]],\n",
    "    'begin': r['begin'],\n",
    "    'end': r['end'],\n",
    "  } for r in ranges]\n",
    "\n",
    "  print(f'tokens: {pformat(tokens)}')\n",
    "\n",
    "  return tokens\n",
    "\n",
    "\n",
    "# def match(words, sentence):\n",
    "#   def lenw(words):\n",
    "#     return sum([len(w) for w in words])\n",
    "\n",
    "#   if len(words) == 0 or len(sentence) == 0:\n",
    "#     return lenw(words) + len(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2497d198-cddd-4495-9530-988dda162680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from speechsynthesis.alitts import create_token, websocket_tts, tts_with_subtitle\n",
    "\n",
    "sentences = [\n",
    "  \"闪耀中装协住宅产业年会，恒洁获2022红鼎奖最高奖\",\n",
    "  '恒洁的花洒和智能一体机都搭载了包括精准控温、长效阻垢、水净、镀层等在内的恒洁“技术包”，让您享受更加智能的卫浴体验。',\n",
    "  'R11智能一体机、T9PRO花洒......恒洁卫浴荣获红鼎奖和红鼎技术TOP10大奖，展示恒洁作为智能卫浴专家的地位。',\n",
    "  '恒洁通过“恒洁选品SHOW”和“设计圈”等工具，与装企合作，提升效率，享受合作共赢的喜悦。让你更轻松、更高效地完成家装。',\n",
    "  '恒洁T9PRO花洒荣膺“红鼎技术TOP10”大奖，恒洁“技术包”让你的卫浴更加智能､舒适。精确控温技术､长效阻垢技术､水净技术……细节成就卓越。',\n",
    "  '恒洁卫浴的创新能力备受肯定。恒洁R11智能一体机斩获红鼎奖最高荣誉，而T9PRO花洒则被评为红鼎技术TOP10。这些荣誉背后，是我们持续追求卓越的信念！',\n",
    "  \"恒洁致力于提供符合品质生活需求的智能卫浴产品和服务，与装企伙伴一同打造品质家装。\",\n",
    "  \"恒洁将国潮红利转化为品牌价值，从消费者视角定义恒洁品质。选恒洁，选品质，就是选生活。\",\n",
    "  \"恒洁以超低水压的Q9X、智感设计的R11等智能卫浴产品为支撑，满足品质生活需求，打造个性化颜值的全卫空间解决方案。\",\n",
    "  \"恒洁拥有完善的综合服务体系和强大的服务保障能力，三千多个网点遍布全国四百多个城市，让全卫焕新仅需3小时，让您享受快速暖心的服务体验。\",\n",
    "  \"恒洁T9PRO花洒荣膺“红鼎技术TOP10”大奖，凭借包括长效阻垢技术在内的恒洁“技术包”，满足消费者对卫浴产品的高品质需求。\",\n",
    "  \"恒洁R11智能一体机凭借完美的美学特征和高端智能技术斩获最高奖红鼎奖，为住宅产业注入更多创新和设计思维，让品质人居变得更加美好。\"\n",
    "]\n",
    "\n",
    "token = create_token()[0]\n",
    "metadata = []\n",
    "\n",
    "for i, s in enumerate(sentences):\n",
    "  _, m = await websocket_tts(\n",
    "    text=s,\n",
    "    voice='kenny',\n",
    "    wav_file=os.path.join('/Volumes/RamDisk/temp', f'{i}.wav'),\n",
    "    token=token\n",
    "  )\n",
    "  metadata.append(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85e769d-a598-4306-af36-2210e8ac9a5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "ssa_line_width = 12\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Token:\n",
    "  text: str = ''\n",
    "  begin: int = -1\n",
    "  end: int = -1\n",
    "  omitted: bool = False\n",
    "  tts_sent_break: bool = False\n",
    "  tts_tok_idx: int = -1\n",
    "  nlp_sent: bool = False\n",
    "  nlp_tok_idx: int = -1\n",
    "  punct = None\n",
    "\n",
    "\n",
    "def is_left_punct(str):\n",
    "  punc = r'^\\s*[“‘（［｛〔《「『【〖〘〚〝﹙﹛﹝（｛［‘“].*'  # -—_…\n",
    "  return re.match(punc, str) is not None\n",
    "\n",
    "\n",
    "def is_sent_sep(str):\n",
    "  conds = [\n",
    "    r'.*\\\\n\\s*$',\n",
    "    r'.*\\\\N\\s*$',\n",
    "  ]\n",
    "  return re.match('|'.join(conds), str) is not None\n",
    "\n",
    "\n",
    "def split_sent_sep(str):\n",
    "  sent_sep = ['。', '！', '？', '\\\\\\\\n', '\\\\\\\\N']\n",
    "  pattern = \\\n",
    "      r'(' + '|'.join(sent_sep) + r')\\s*$' + r'|' + \\\n",
    "      r'^\\s*(' + '|'.join(sent_sep) + r')'\n",
    "  return [x for x in re.split(pattern, str) if x]\n",
    "\n",
    "# def split_sent_sep(str):\n",
    "#   return [x for x in re.split(r'(\\\\n\\s*$|\\\\N\\s*$)', str) if x != '']\n",
    "\n",
    "\n",
    "def sent_sep(text):\n",
    "  # punc = \"！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.\"\n",
    "  # punc = punc.decode(\"utf-8\")\n",
    "\n",
    "  hard_sep = [\n",
    "    r'([。！？\\?](?=$|[^”’]))',\n",
    "    r'(\\.{6}(?=[^”’]))|(\\.{3}(?=[^”’]))',\n",
    "    r'(\\…{1,2}(?=[^”’]))',\n",
    "    r'([。！？\\?][”’][^，。！？\\?])',\n",
    "  ]\n",
    "\n",
    "  soft_sep = [\n",
    "    r'([,，；](?=$|[^”’]))',\n",
    "    r'([。！？\\?][”’][^，。！？\\?])',\n",
    "  ]\n",
    "\n",
    "  text = re.sub('|'.join(hard_sep), r'\\\\N', text)\n",
    "  text = re.sub('|'.join(soft_sep), r'\\\\n', text)\n",
    "  return text\n",
    "  # results = re.finditer('|'.join(sent_sep), s)\n",
    "  # for ret in results:\n",
    "  #   print(f\"Punct '{ret.group()}' at index ({ret.start()}, {ret.end()})\")\n",
    "\n",
    "\n",
    "def tokenize(ts, sentence):\n",
    "  a = ''.join([t['text'] for t in ts])\n",
    "  print(a)\n",
    "  print(sentence)\n",
    "\n",
    "  s = SequenceMatcher(None, a, sentence)\n",
    "\n",
    "  ts_idx = 0\n",
    "  char_idx = 0\n",
    "  tok_idx = 0\n",
    "  tokens = []\n",
    "  for tag, i1, i2, j1, j2 in s.get_opcodes():\n",
    "    print('{:7}   a[{}:{}] --> b[{}:{}] {!r:>8} --> {!r}'.format(\n",
    "      tag, i1, i2, j1, j2, a[i1:i2], sentence[j1:j2]))\n",
    "\n",
    "    # 'delete' tag means TTS engine introduces new token that not belong to the origin sentence\n",
    "    #  we can not drop them, so treat the newly created as 'equal'\n",
    "    if tag == 'equal' or tag == 'delete':\n",
    "      while char_idx < i2:\n",
    "        t = ts[ts_idx]\n",
    "        token = tokens.append(Token(\n",
    "          text=t['text'],\n",
    "          begin=t['begin_time'],\n",
    "          end=t['end_time'],\n",
    "          tts_tok_idx=tok_idx,\n",
    "        ))\n",
    "        char_idx += len(t['text'])\n",
    "        ts_idx += 1\n",
    "\n",
    "        if ts_idx < len(ts) and t['end_index'] != ts[ts_idx]['begin_index']:\n",
    "          tok_idx += 1\n",
    "    elif tag == 'replace':\n",
    "      # merge as one token\n",
    "      # if the last tag was `equal`, there might be an overflow since t['text'] might be word\n",
    "      # e.g. if the tts gives `top十佳` and the origin sentence is `toop10佳`\n",
    "      # the match result will be { equal: 'to', replace: ['p十','op10'], equal: '佳' }\n",
    "      # but in this case, from the tss side, `top` is a word, and we should honor that.\n",
    "      # there for:\n",
    "      # 1. when dealing with the first `equal` part, `top` will be taken as a token, though 'p'\n",
    "      #    is not part of the equal tag, only 'to' is;\n",
    "      # 2. when dealing with `replace`, the `p` of 'p十' will be skip since it has been taken by\n",
    "      #    token `top` in the first step, `op10`(from the origin sentence) will be created as a\n",
    "      #    token, which will take the `begin_time` and `end_time` of `十` (`p十` without `p`)\n",
    "\n",
    "      # HACK! split the sentence break into separate tokens if any\n",
    "      texts = split_sent_sep(sentence[j1:j2])\n",
    "      for text in texts:\n",
    "        token = Token(text)\n",
    "        if is_sent_sep(text):\n",
    "          tokens.append(token)\n",
    "        else:\n",
    "          while char_idx < i2:\n",
    "            t = ts[ts_idx]\n",
    "            if token.begin == -1:\n",
    "              token.begin = t['begin_time']\n",
    "            token.end = t['end_time']\n",
    "            char_idx += len(t['text'])\n",
    "            ts_idx += 1\n",
    "          if token.text != '':\n",
    "            tokens.append(token)\n",
    "    elif tag == 'insert':\n",
    "      tokens.append(Token(\n",
    "        text=sentence[j1:j2],\n",
    "        omitted=True,\n",
    "      ))\n",
    "      # for j in range(j1, j2):\n",
    "      #   tokens.append(Token(\n",
    "      #     text=sentence[j],\n",
    "      #     omitted=True,\n",
    "      #   ))\n",
    "    else:\n",
    "      raise RuntimeError(f'Unknown tag: {tag}')\n",
    "\n",
    "  return tokens\n",
    "\n",
    "\n",
    "def wrap(texts):\n",
    "  total_len = sum([len(t) for t in texts])\n",
    "  n = round(total_len / ssa_line_width)\n",
    "  if n < 2:\n",
    "    return ''.join(texts)\n",
    "\n",
    "  avg_line_width = round(total_len / n)\n",
    "  length = 0\n",
    "  text = ''\n",
    "  for t in texts:\n",
    "    if length > avg_line_width:\n",
    "      text += '\\\\N'\n",
    "      length = 0\n",
    "    text += t\n",
    "    length += len(t)\n",
    "  return text\n",
    "\n",
    "\n",
    "def wording(metadata, sentence):\n",
    "  tokens = tokenize(metadata['payload']['subtitles'], sent_sep(sentence))\n",
    "  # print(pformat([t for t in tokens]))\n",
    "\n",
    "  candidates = [(t, i) for i, t in enumerate(tokens) if t.begin == -1]\n",
    "  for item, idx in candidates:\n",
    "    if is_left_punct(item.text) and idx < len(tokens) - 1:\n",
    "      tokens[idx + 1].text = item.text + tokens[idx + 1].text\n",
    "    elif is_sent_sep(item.text):\n",
    "      item.tts_sent_break = True\n",
    "    elif idx > 0:\n",
    "      tokens[idx - 1].text = tokens[idx - 1].text + item.text\n",
    "  tokens = [t for t in tokens if t.begin != -1 or t.tts_sent_break]\n",
    "  # print(len(tokens))\n",
    "  # print([t.text for t in tokens])\n",
    "\n",
    "  # groups =[list(g) for _, g in groupby(tokens, lambda x: f'{len(x.text)}')]\n",
    "  stens = [list(g) for _, g in groupby(tokens, lambda x: x.tts_sent_break)]\n",
    "  stens = [s for s in stens if not s[0].tts_sent_break]\n",
    "  for sten in stens:\n",
    "    groups = [list(g) for _, g in groupby(sten, lambda x: x.tts_tok_idx)]\n",
    "    words = [Token(\n",
    "      text=''.join([i.text for i in g]),\n",
    "      begin=g[0].begin,\n",
    "      end=g[-1].end,\n",
    "    ) for g in groups]\n",
    "    print(\n",
    "      f'begin: {words[0].begin}, end: {words[-1].end}, text: {wrap([w.text for w in words])}')\n",
    "\n",
    "# idx = 3\n",
    "# wording(metadata[idx], sentences[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d205736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/Users/zhengt/Codes/VideoClipGen/data/20230411-140813/'\n",
    "\n",
    "ts_path = Path(os.path.join(data, 'wav'))\n",
    "ts_files = sorted([str(p) for p in ts_path.glob('**/*.json')])\n",
    "\n",
    "summaries = json.load(open(os.path.join(data, 'summaries.json')))\n",
    "sentences = summaries['summaries']\n",
    "\n",
    "for idx, sentence in enumerate(sentences):\n",
    "  ts = json.load(open(ts_files[idx], 'r'))\n",
    "  wording(ts, sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaca196-632c-44c1-a615-9d7136e4f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/hankcs/HanLP/blob/master/hanlp/utils/rules.py\n",
    "\n",
    "import re\n",
    "\n",
    "SEPARATOR = r'@'\n",
    "RE_SENTENCE = re.compile(r'(\\S.+?[.!?])(?=\\s+|$)|(\\S.+?)(?=[\\n]|$)', re.UNICODE)\n",
    "AB_SENIOR = re.compile(r'([A-Z][a-z]{1,2}\\.)\\s(\\w)', re.UNICODE)\n",
    "AB_ACRONYM = re.compile(r'(\\.[a-zA-Z]\\.)\\s(\\w)', re.UNICODE)\n",
    "UNDO_AB_SENIOR = re.compile(\n",
    "  r'([A-Z][a-z]{1,2}\\.)' + SEPARATOR + r'(\\w)', re.UNICODE)\n",
    "UNDO_AB_ACRONYM = re.compile(\n",
    "  r'(\\.[a-zA-Z]\\.)' + SEPARATOR + r'(\\w)', re.UNICODE)\n",
    "\n",
    "\n",
    "def replace_with_separator(text, separator, regexs):\n",
    "  replacement = r\"\\1\" + separator + r\"\\2\"\n",
    "  result = text\n",
    "  for regex in regexs:\n",
    "    result = regex.sub(replacement, result)\n",
    "  return result\n",
    "\n",
    "\n",
    "def split_sent_sep(text, best=True):\n",
    "  text = re.sub('([。！？\\?])([^”’])', r\"\\1\\n\\2\", text)\n",
    "  text = re.sub('(\\.{6})([^”’])|(\\.{3})([^”’])', r\"\\1\\n\\2\", text)\n",
    "  text = re.sub('(\\…{1,2})([^”’])', r\"\\1\\n\\2\", text)\n",
    "  text = re.sub('([。！？\\?][”’])([^，。！？\\?])', r'\\1\\n\\2', text)\n",
    "  for chunk in text.split(\"\\n\"):\n",
    "    chunk = chunk.strip()\n",
    "    if not chunk:\n",
    "      continue\n",
    "    if not best:\n",
    "      yield chunk\n",
    "      continue\n",
    "    processed = replace_with_separator(\n",
    "      chunk, SEPARATOR, [AB_SENIOR, AB_ACRONYM])\n",
    "    for sentence in RE_SENTENCE.finditer(processed):\n",
    "      sentence = replace_with_separator(\n",
    "        sentence.group(), r\" \", [UNDO_AB_SENIOR, UNDO_AB_ACRONYM])\n",
    "      yield sentence\n",
    "\n",
    "\n",
    "def find_punct(s):\n",
    "  sent_sep = [\n",
    "    '([。！？\\?](?=$|[^”’]))',\n",
    "    '(\\.{6}(?=[^”’]))|(\\.{3}(?=[^”’]))',\n",
    "    '(\\…{1,2}(?=[^”’]))',\n",
    "    '([。！？\\?][”’][^，。！？\\?])',\n",
    "  ]\n",
    "\n",
    "  results = re.finditer('|'.join(sent_sep), s)\n",
    "  for ret in results:\n",
    "    print(f\"Punct '{ret.group()}' at index ({ret.start()}, {ret.end()})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6f2541-4574-4449-b0be-be7f71cab110",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sentences:\n",
    "  print(s)\n",
    "  find_punct(s)\n",
    "\n",
    "for s in sentences:\n",
    "  print([a for a in split_sent_sep(s, best=False)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6ac23d-31e9-49e4-986b-1a7cdf227d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = 'Prof. White'\n",
    "print(re.split('([A-Z][a-z]{1,3}\\.\\s)', 'Prof. White'))\n",
    "\n",
    "# punctuations = re.finditer(r\"\\.{3,6}\", text)\n",
    "# r\"\\.{3,6}\"\n",
    "# r\"\\.{6}|\\.{3}\"\n",
    "\n",
    "# punctuations = re.finditer('([A-Z][a-z]{1,3}\\.\\s)|\\.{3,6}', text)\n",
    "# for p in punctuations:\n",
    "#   print(f\"Punctuation '{p.group()}' at index {p.start()}, {p.end()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05d1f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sent_sep = ['。', '！', '？', '\\\\\\\\n', '\\\\\\\\N']\n",
    "\n",
    "\n",
    "def is_sent_sep(str):\n",
    "  pattern = r'.*(' + '|'.join(sent_sep) + r')\\s*$'\n",
    "  print(pattern)\n",
    "  return re.match(pattern, str) is not None\n",
    "\n",
    "\n",
    "def is_sent_sep(str):\n",
    "  return re.match(r'.*(。|！|？|\\\\n|\\\\N)\\s*$', str) is not None\n",
    "\n",
    "\n",
    "print(is_sent_sep(' \\\\n '))\n",
    "print(is_sent_sep('aa\\\\n'))\n",
    "print(is_sent_sep('\\\\naa'))\n",
    "print(is_sent_sep('  \\\\N '))\n",
    "print(is_sent_sep('aa\\\\N '))\n",
    "print(is_sent_sep('\\\\Naa'))\n",
    "print(is_left_punct(' “ '))\n",
    "print(is_left_punct('“aa'))\n",
    "print(is_left_punct('aa“'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d593098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sent_sep(str):\n",
    "  return [x for x in re.split(r'(' + r'[。！？\\\\n\\\\N]' + '\\s*$)', str) if x != '']\n",
    "\n",
    "\n",
    "split_sent_sep('aa\\\\N')\n",
    "split_sent_sep('aa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f71834f-f5c2-4c00-bec6-5b1c2008339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "  '''\n",
    "  Remove Chinese and English punctuation from text, as well as all spaces.\n",
    "  '''\n",
    "  # Define regex patterns for Chinese and English punctuation\n",
    "  # symbol_pattern = '\\\\s+'\n",
    "  # chs_punct_pattern = '[\\u3000-\\u303f\\uFF00-\\uFFEF]'\n",
    "  # eng_punct_pattern = '[\\u0021-\\u002f\\u003a-\\u0040\\u005b-\\u0060\\u007b-\\u007e]'\n",
    "  # Remove Chinese and English punctuation using regex\n",
    "  # return re.sub('|'.join([\n",
    "  #   symbol_pattern,\n",
    "  #   chs_punct_pattern,\n",
    "  #   eng_punct_pattern,\n",
    "  # ]), '', text)\n",
    "  return re.sub(r'[^\\w]', '', text)\n",
    "\n",
    "\n",
    "def tok(timestamps):\n",
    "  '''\n",
    "  Tokenize the timestamps into words\n",
    "  1. combine consecutive indexed characters into words\n",
    "  2. unless the timestamp of two characters are not consecutive\n",
    "  '''\n",
    "\n",
    "  # assemble words based on `begin_index` and `end_index`\n",
    "  words = [{'text': '', 'begin': 0, 'end': 0}]\n",
    "  index = 0\n",
    "  for t in timestamps:\n",
    "    # assuming the `begin_time` of the first item is 0\n",
    "    if t['begin_index'] == index \\\n",
    "            and t['begin_time'] - words[-1]['end'] < word_break_ms:\n",
    "      words[-1]['text'] += t['text']\n",
    "      words[-1]['end'] = t['end_time']\n",
    "    else:\n",
    "      words.append({\n",
    "        'text': t['text'],\n",
    "        'begin': t['begin_time'],\n",
    "        'end': t['end_time']\n",
    "      })\n",
    "    index = t['end_index']\n",
    "\n",
    "  return words\n",
    "\n",
    "\n",
    "def mapping(words, sentence):\n",
    "  '''\n",
    "  tokenize the sentence by aligning the summary sentence with given words\n",
    "  '''\n",
    "\n",
    "  logging.info(f'sentence: {sentence}')\n",
    "  logging.info(f'words: {pformat(words)}')\n",
    "\n",
    "  # matching all words from the beginning of the sentence\n",
    "  # skip the words that cannot be matched, fill the gap later\n",
    "  pointer = 0\n",
    "  ranges = [{'range': (0, 0), 'begin': 0, 'end': 0}]\n",
    "  for w in words:\n",
    "    l = len(w['text'])\n",
    "    if ranges[-1]['range'] is not None:\n",
    "      # sentence and words are currently matched before dealing with word w\n",
    "      if w['text'] == sentence[pointer:pointer + l]:\n",
    "        # w and sentence are matched at (pointer, pointer + l) of sentence\n",
    "        ranges.append({\n",
    "          'range': (pointer, pointer + l),\n",
    "          'begin': w['begin'],\n",
    "          'end': w['end'],\n",
    "        })\n",
    "        pointer += l\n",
    "      else:\n",
    "        # w and sentence are not matched, record the timestamp\n",
    "        ranges.append({\n",
    "          'range': None,\n",
    "          'begin': w['begin'],\n",
    "          'end': w['end'],\n",
    "        })\n",
    "    else:\n",
    "      # sentence and words are NOT matched before dealing with word w\n",
    "      if (p := sentence[pointer:].find(w['text'])) > -1:\n",
    "        # w and sentence are matched, at position `p` from `pointer`\n",
    "        pointer += p\n",
    "        ranges.append({\n",
    "          'range': (pointer, pointer + l),\n",
    "          'begin': w['begin'],\n",
    "          'end': w['end'],\n",
    "        })\n",
    "        pointer += l\n",
    "      else:\n",
    "        # still no match, extend the timestamp\n",
    "        ranges[-1]['end'] = w['end']\n",
    "\n",
    "  # drop the placeholder\n",
    "  ranges = ranges[1:]\n",
    "  logging.info(f'ranges: {pformat(ranges)}')\n",
    "\n",
    "  for i, r in enumerate(ranges):\n",
    "    if r['range'] is None:\n",
    "      start = ranges[i - 1]['range'][1] if i > 0 else 0\n",
    "      end = ranges[i + 1]['range'][0] if i < len(ranges) - 1 else len(sentence)\n",
    "      ranges[i]['range'] = (start, end)\n",
    "\n",
    "  tokens = [{\n",
    "    'text': sentence[r['range'][0]:r['range'][1]],\n",
    "    'begin': r['begin'],\n",
    "    'end': r['end'],\n",
    "  } for r in ranges]\n",
    "\n",
    "  logging.info(f'tokens: {pformat(tokens)}')\n",
    "\n",
    "  return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d26a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sent_sep(str):\n",
    "  sent_sep = ['。', '！', '？', '\\\\\\\\n', '\\\\\\\\N']\n",
    "  pattern = \\\n",
    "      r'(' + '|'.join(sent_sep) + r')\\s*$' + r'|' + \\\n",
    "      r'^\\s*(' + '|'.join(sent_sep) + r')'\n",
    "  return [x for x in re.split(pattern, str) if x]\n",
    "\n",
    "\n",
    "print(split_sent_sep('aa\\\\N'))\n",
    "print(split_sent_sep('\\\\N2017'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471ddc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_sep_in_quote(text):\n",
    "  hard_sep = [\n",
    "    r'([｡。！？\\?](?=$|[^”’]))',\n",
    "    r'([。！？?][”’](?=[^，。！？?]))',\n",
    "  ]\n",
    "\n",
    "  soft_sep = [\n",
    "    r'([,，；](?=$|[^”’]))',\n",
    "  ]\n",
    "\n",
    "  text = re.sub('|'.join(hard_sep), r'\\\\N', text)\n",
    "  text = re.sub('|'.join(soft_sep), r'\\\\n', text)\n",
    "\n",
    "  return text\n",
    "\n",
    "\n",
    "print(replace_sep_in_quote('a？b'))\n",
    "print(replace_sep_in_quote('a？”b'))\n",
    "print(replace_sep_in_quote('a？”.b'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49d17b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_sep = [r'。', r'！', r'？', r'\\\\n', r'\\\\N']\n",
    "\n",
    "\n",
    "def has_sent_sep(str):\n",
    "  pattern = r'.*[' + '|'.join(sent_sep) + r']\\s*$'\n",
    "  return re.match(pattern, str) is not None\n",
    "\n",
    "\n",
    "print(has_sent_sep('。a'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe8d6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechsynthesis.subtitle import (\n",
    "  tokenize,\n",
    "  wording,\n",
    "  subtitle,\n",
    ")\n",
    "\n",
    "\n",
    "def sub():\n",
    "  import json\n",
    "  data = '/Users/zhengt/Codes/VideoClipGen/data/20230411-134744/'\n",
    "\n",
    "  ts_path = Path(os.path.join(data, 'wav'))\n",
    "  ts_files = sorted([str(p) for p in ts_path.glob('**/*.json')])\n",
    "\n",
    "  summaries = json.load(open(os.path.join(data, 'summaries.json')))\n",
    "  sentences = summaries['summaries']\n",
    "\n",
    "  for idx, sentence in enumerate(sentences):\n",
    "    ts = json.load(open(ts_files[idx], 'r'))\n",
    "    tokens = tokenize(\n",
    "      ts=ts['payload']['subtitles'],\n",
    "      sentence=sentence,\n",
    "    )\n",
    "    lines = wording(tokens)\n",
    "    subtitles = subtitle(lines, (720, 1280))\n",
    "    print(subtitles)\n",
    "\n",
    "\n",
    "sub()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
